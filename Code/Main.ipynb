{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa722363",
   "metadata": {},
   "source": [
    "# PsyPredict: ML Framework for Mental Health Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c7e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging, re, hashlib\n",
    "from typing import Dict\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3cf2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"ml_preprocessing_v4\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f60501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\AKHILESH ANIL\\AppData\\Local\\Temp\\ipykernel_25264\\3450116034.py:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  DATA_PATH = \"D:\\Programming Languages\\Machine Learning\\Projects\\PsyPredict\\Data\\mental_disorders_dataset.csv\"\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"D:\\Programming Languages\\Machine Learning\\Projects\\PsyPredict\\Data\\mental_disorders_dataset.csv\"\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "EMAIL_REGEX = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "PHONE_REGEX = re.compile(r\"(\\+?\\d{1,3}[\\s-]?)?(\\(?\\d{2,4}\\)?[\\s-]?)?\\d{6,12}\")\n",
    "PII_COLUMN_KEYWORDS = [\"name\", \"email\", \"phone\", \"address\", \"ssn\", \"id\", \"userid\", \"patient number\", \"patient_number\", \"patientid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a308eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_hash_series(s: pd.Series, salt: str):\n",
    "    return s.fillna(\"\").astype(str).apply(lambda x: hashlib.sha256((salt + x).encode(\"utf-8\")).hexdigest())\n",
    "\n",
    "def detect_pii_columns(df: pd.DataFrame, sample_size: int = 200) -> Dict[str, str]:\n",
    "    reasons = {}\n",
    "    for col in df.columns:\n",
    "        lname = col.lower()\n",
    "        if any(k in lname for k in PII_COLUMN_KEYWORDS):\n",
    "            reasons[col] = f\"column name '{col}' contains PII keyword\"\n",
    "            continue\n",
    "        sample = df[col].dropna().astype(str).head(sample_size).tolist()\n",
    "        if any(EMAIL_REGEX.search(x) for x in sample):\n",
    "            reasons[col] = \"contains email-like values\"\n",
    "            continue\n",
    "        if any(PHONE_REGEX.search(x) for x in sample):\n",
    "            reasons[col] = \"contains phone-like values\"\n",
    "            continue\n",
    "    return reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf75f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WinsorizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "    def fit(self, X, y=None):\n",
    "        import pandas as _pd\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = _pd.DataFrame(X)\n",
    "        self.lower_bounds_ = X.quantile(self.lower_quantile)\n",
    "        self.upper_bounds_ = X.quantile(self.upper_quantile)\n",
    "        self.feature_names_in_ = list(X.columns)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pandas as _pd\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = _pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "        X_w = X.copy()\n",
    "        for col in X_w.columns:\n",
    "            low = self.lower_bounds_.loc[col]\n",
    "            high = self.upper_bounds_.loc[col]\n",
    "            X_w[col] = X_w[col].clip(lower=low, upper=high)\n",
    "        return X_w.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3866aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        import pandas as _pd\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = _pd.DataFrame(X)\n",
    "        self.maps_ = {}\n",
    "        for col in X.columns:\n",
    "            vc = X[col].fillna(\"__MISSING__\").astype(str).value_counts(normalize=True)\n",
    "            self.maps_[col] = vc.to_dict()\n",
    "        self.feature_names_in_ = list(X.columns)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pandas as _pd\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = _pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "        out = _pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            map_ = self.maps_.get(col, {})\n",
    "            out[col] = X[col].fillna(\"__MISSING__\").astype(str).map(map_).fillna(0.0)\n",
    "        return out.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e5e07",
   "metadata": {},
   "source": [
    "### Step 1: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68b7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:11,911 - INFO - Loaded dataset shape: (120, 19)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "logger.info(f\"Loaded dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144662b1",
   "metadata": {},
   "source": [
    "### Step 2: Detect Personal Identification IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c34ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:11,921 - INFO - PII-like columns detected (pre-clean): ['Patient Number', 'Suicidal thoughts']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_pii = detect_pii_columns(df)\n",
    "logger.info(f\"PII-like columns detected (pre-clean): {list(initial_pii.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e13d5b",
   "metadata": {},
   "source": [
    "### Step 3A: Drop IDs for Anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6edd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:11,931 - INFO - Dropping id-like columns: ['Patient Number']\n"
     ]
    }
   ],
   "source": [
    "nrows = df.shape[0]\n",
    "id_like = [c for c in df.columns if (df[c].nunique()/max(1,nrows) > 0.99) and (\"patient\" in c.lower() or c.lower().startswith(\"id\") or \"number\" in c.lower())]\n",
    "if id_like:\n",
    "    logger.info(f\"Dropping id-like columns: {id_like}\")\n",
    "    df.drop(columns = id_like, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9075a",
   "metadata": {},
   "source": [
    "### Step 3B: Pseudonymize PII and Drop original unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b259216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:11,943 - INFO - Pseudonymizing columns: ['Suicidal thoughts']\n",
      "2025-09-28 19:39:11,945 - INFO - Dropped original PII column: Suicidal thoughts\n"
     ]
    }
   ],
   "source": [
    "pii_reasons = detect_pii_columns(df)\n",
    "if pii_reasons:\n",
    "    logger.info(f\"Pseudonymizing columns: {list(pii_reasons.keys())}\")\n",
    "    salt = os.environ.get(\"PII_HASH_SALT\", \"static_demo_salt_change_in_prod\")  \n",
    "    for c in pii_reasons:\n",
    "        df[f\"__hashed__{c}\"] = safe_hash_series(df[c], salt=salt)\n",
    "    for c in pii_reasons:\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "            logger.info(f\"Dropped original PII column: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8206279",
   "metadata": {},
   "source": [
    "### Step 4: Target detection & Excluding hashed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e90523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:11,952 - INFO - Inferred target: Expert Diagnose\n"
     ]
    }
   ],
   "source": [
    "candidates = [c for c in df.columns if (not c.startswith(\"__hashed__\")) and c.lower() in (\"target\", \"label\", \"diagnosis\", \"outcome\", \"mental_disorder\", \"has_disorder\", \"class\")]\n",
    "candidates += [c for c in df.columns if (not c.startswith(\"__hashed__\")) and df[c].nunique() <= 10 and c.lower().startswith(\"y_\")]\n",
    "candidates = list(dict.fromkeys(candidates))\n",
    "target_col = candidates[0] if candidates else None\n",
    "if not target_col:\n",
    "    for c in df.columns[::-1]:\n",
    "        if c.startswith(\"__hashed__\"): continue\n",
    "        if df[c].nunique() <= 5 and df[c].nunique() > 1 and c.lower() not in ['id', 'patient_id']:\n",
    "            target_col = c\n",
    "            break\n",
    "logger.info(f\"Inferred target: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125756c7",
   "metadata": {},
   "source": [
    "### Step 5: Create feature matrix X & Drop hashed PII from X by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae49d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:11,961 - INFO - Dropping hashed PII columns from features: ['__hashed__Suicidal thoughts']\n"
     ]
    }
   ],
   "source": [
    "if target_col:\n",
    "    X = df.drop(columns=[target_col]).copy()\n",
    "    y = df[target_col].copy()\n",
    "else:\n",
    "    X = df.copy()\n",
    "    y = None\n",
    "hashed_cols = [c for c in X.columns if c.startswith(\"__hashed__\")]\n",
    "if hashed_cols:\n",
    "    logger.info(f\"Dropping hashed PII columns from features: {hashed_cols}\")\n",
    "    X.drop(columns = hashed_cols, inplace = True)\n",
    "# Drop any remaining id-like in X\n",
    "id_like_2 = [c for c in X.columns if (X[c].nunique()/max(1,X.shape[0]) > 0.99) and (\"patient\" in c.lower() or c.lower().startswith(\"id\") or \"number\" in c.lower())]\n",
    "if id_like_2:\n",
    "    logger.info(f\"Dropping id-like in features: {id_like_2}\")\n",
    "    X.drop(columns=id_like_2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20714e8",
   "metadata": {},
   "source": [
    "### Step 6: Determine dtypes & missingness and add missing flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5c8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "null_percent = (X.isnull().mean()*100).sort_values(ascending=False)\n",
    "numeric_small_missing = [c for c in num_cols if X[c].isnull().mean() < 0.05]\n",
    "numeric_medium_missing = [c for c in num_cols if 0.05 <= X[c].isnull().mean() <= 0.30]\n",
    "numeric_large_missing = [c for c in num_cols if X[c].isnull().mean() > 0.30]\n",
    "cat_small_missing = [c for c in cat_cols if X[c].isnull().mean() < 0.05]\n",
    "cat_large_missing = [c for c in cat_cols if X[c].isnull().mean() >= 0.05]\n",
    "for c in numeric_medium_missing + numeric_large_missing + cat_large_missing:\n",
    "    X[f\"__missing_flag__{c}\"] = X[c].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe518d7",
   "metadata": {},
   "source": [
    "### Step 7: Build ColumnTransformer from final X's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e27fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = [c for c in cat_cols if X[c].nunique() <= 20]\n",
    "freq_cols = [c for c in cat_cols if X[c].nunique() > 20]\n",
    "numeric_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"winsor\", WinsorizerTransformer()), (\"scaler\", StandardScaler())])\n",
    "ohe_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "freq_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"freq\", FrequencyEncoder())])\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", numeric_pipeline, num_cols))\n",
    "if ohe_cols:\n",
    "    transformers.append((\"ohe\", ohe_pipeline, ohe_cols))\n",
    "if freq_cols:\n",
    "    transformers.append((\"freq\", freq_pipeline, freq_cols))\n",
    "if not transformers:\n",
    "    raise RuntimeError(\"No transformers configured for these features\")\n",
    "preprocessor = ColumnTransformer(transformers=transformers, sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88572fe2",
   "metadata": {},
   "source": [
    "### Step 8: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3812bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y is not None:\n",
    "    stratify = None\n",
    "    try:\n",
    "        vc = y.value_counts()\n",
    "        if vc.min() >= 2:\n",
    "            stratify = y\n",
    "        else:\n",
    "            logger.warning(\"Not enough class members to stratify; splitting without stratify\")\n",
    "    except Exception:\n",
    "        logger.warning(\"Stratify check failed; splitting without stratify\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=stratify)\n",
    "else:\n",
    "    X_train = X_test = X.copy()\n",
    "    y_train = y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c777b4",
   "metadata": {},
   "source": [
    "### Step 9: Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c794da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:12,010 - INFO - Transformed training shape: (96, 60)\n"
     ]
    }
   ],
   "source": [
    "preprocessor.fit(X_train)\n",
    "X_train_t = preprocessor.transform(X_train)\n",
    "logger.info(f\"Transformed training shape: {X_train_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fba7de",
   "metadata": {},
   "source": [
    "### Step 10: Model Preparation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2c1bb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:39:12,106 - INFO - CV finished. Mean accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 5-fold cross-validation (this may take a moment)...\n",
      "\n",
      "Cross-validation results (per fold):\n",
      " fold  accuracy  f1_weighted  precision_weighted  recall_weighted\n",
      "    1  0.833333     0.832993            0.848810         0.833333\n",
      "    2  0.916667     0.915449            0.927827         0.916667\n",
      "    3  0.791667     0.786713            0.792857         0.791667\n",
      "    4  0.875000     0.869872            0.886905         0.875000\n",
      "    5  0.833333     0.830769            0.880952         0.833333\n",
      "\n",
      "Cross-validation summary (mean ± std):\n",
      "- accuracy: 0.8500 ± 0.0475\n",
      "- f1_weighted: 0.8472 ± 0.0482\n",
      "- precision_weighted: 0.8675 ± 0.0503\n",
      "- recall_weighted: 0.8500 ± 0.0475\n",
      "\n",
      "Holdout accuracy: 0.8750\n",
      "\n",
      "Classification report (holdout):\n",
      "                 precision  recall  f1-score  support\n",
      "class_or_metric                                      \n",
      "Bipolar Type-1      1.0000  0.8333    0.9091    6.000\n",
      "Bipolar Type-2      0.7500  1.0000    0.8571    6.000\n",
      "Depression          0.8571  1.0000    0.9231    6.000\n",
      "Normal              1.0000  0.6667    0.8000    6.000\n",
      "accuracy            0.8750  0.8750    0.8750    0.875\n",
      "macro avg           0.9018  0.8750    0.8723   24.000\n",
      "weighted avg        0.9018  0.8750    0.8723   24.000\n",
      "\n",
      "Step (10) complete — printed CV and holdout results.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logger = logging.getLogger(\"step10_output\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "if 'y' not in globals() or y is None:\n",
    "    print(\"No target (y) provided — skipping Step (10).\")\n",
    "else:\n",
    "    try:\n",
    "        clf = LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            random_state=RANDOM_SEED,\n",
    "            class_weight=\"balanced\",\n",
    "            solver=\"saga\",           \n",
    "            multi_class = \"multinomial\",\n",
    "            n_jobs=None\n",
    "        )\n",
    "        pipeline = Pipeline([(\"preproc\", preprocessor), (\"clf\", clf)])\n",
    "        scoring = [\"accuracy\", \"f1_weighted\", \"precision_weighted\", \"recall_weighted\"]\n",
    "        print(\"\\nRunning 5-fold cross-validation (this may take a moment)...\")\n",
    "        cv_res = cross_validate(\n",
    "            pipeline,\n",
    "            X,\n",
    "            y,\n",
    "            cv=5,\n",
    "            scoring=scoring,\n",
    "            n_jobs=1,\n",
    "            return_train_score=False\n",
    "        )\n",
    "        cv_df = pd.DataFrame({\n",
    "            \"fold\": np.arange(1, len(cv_res[\"test_accuracy\"]) + 1),\n",
    "            \"accuracy\": cv_res[\"test_accuracy\"],\n",
    "            \"f1_weighted\": cv_res[\"test_f1_weighted\"],\n",
    "            \"precision_weighted\": cv_res[\"test_precision_weighted\"],\n",
    "            \"recall_weighted\": cv_res[\"test_recall_weighted\"],\n",
    "        })\n",
    "\n",
    "        cv_summary = cv_df[[\"accuracy\", \"f1_weighted\", \"precision_weighted\", \"recall_weighted\"]].agg([\"mean\", \"std\"]).T.round(4)\n",
    "        print(\"\\nCross-validation results (per fold):\")\n",
    "        print(cv_df.to_string(index=False))\n",
    "        print(\"\\nCross-validation summary (mean ± std):\")\n",
    "        for metric in cv_summary.index:\n",
    "            mean = cv_summary.loc[metric, \"mean\"]\n",
    "            std = cv_summary.loc[metric, \"std\"]\n",
    "            print(f\"- {metric}: {mean:.4f} ± {std:.4f}\")\n",
    "        logger.info(\"CV finished. Mean accuracy: %.4f\", cv_res[\"test_accuracy\"].mean())\n",
    "        if ('X_train' in globals() and 'X_test' in globals() and 'y_train' in globals() and 'y_test' in globals()\n",
    "            and X_train is not None and X_test is not None and y_train is not None and y_test is not None):\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            holdout_acc = accuracy_score(y_test, y_pred)\n",
    "            print(f\"\\nHoldout accuracy: {holdout_acc:.4f}\")\n",
    "            report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "            report_df = pd.DataFrame(report_dict).T.round(4).rename_axis(\"class_or_metric\")\n",
    "            print(\"\\nClassification report (holdout):\")\n",
    "            print(report_df.to_string())\n",
    "        else:\n",
    "            print(\"\\nHoldout split not available in this environment; only CV results were computed.\")\n",
    "\n",
    "        print(\"\\nStep (10) complete — printed CV and holdout results.\")\n",
    "    except Exception as exc:\n",
    "        print(\"\\nERROR during Step (10):\", str(exc))\n",
    "        logger.exception(\"Exception in Step (10)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
